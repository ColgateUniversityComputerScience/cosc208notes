# Efficiency: caching (continued); Multiprocessing: operating systems
_COSC 208, Introduction to Computer Systems, 2021-11-03_

## Announcements
* Project 2 Part B due date extended to Tues, Nov 9

## Outline
* Warm-up
* Cache replacement
* OS overview
* Accessing hardware

## Warm-up
Q1: _For each of the following instances of caching, indicate whether the caching is motivated by temporal or spatial locality._
* A CPU caches the first 32 instructions of a function when the function is called — spatial
* A CPU caches all of the instructions for a frequently called function — temporal
* A web browser caches the Moodle pages for your courses, which you view multiple times per week — temporal
* A content distribution network (CDN) caches a video that has gone viral — temporal
* A content distribution network (CDN) caches "recommended videos" related to a popular video — spatial

## Cache replacement
* If a cache is full, then a cache entry must be removed so different data can be placed in the cache
* Cache replacement policy governs which data is removed
* _What should a good cache replacement policy do?_ — maximize the number of cache hits (or minimize the number of cache misses)
    * Evaluation metric: Hit ratio = number of hits / total number of memory accesses
* _How do we determine which cache entry to replace?_
* Optimal replacement policy: policy that replaces the entry that will be accessed furthest in the future
    * Impractical because we don’t know data access patterns a priori
* First-in First-out (FIFO)
    * Simple to implement
    * Doesn’t consider the importance of a cache entry
* Random
    * Even simpler to implement
    * Doesn’t consider the importance of a cache entry
* Least Frequently Used (LFU) and Least Recently Used (LRU)
    * Based on the principle of locality 
    * LFU assumes a page that is accessed many times will be accessed many more times
    * LRU assumes a page that was accessed recently will be accessed again soon
    * Inverse is very bad replacement policy
    * Downside: lots of overhead to implement — need to store an ordered list of pages and move a page up in the list whenever it’s accessed
    * Where does this go wrong? — when working-set size (i.e., number of repeatedly accessed entries) is (slightly) greater than size of the cache
* _Assume a cache can hold 3 entries and the following 15 data accesses occur: 3, 4, 4, 5, 3, 2, 3, 4, 1, 4, 4, 2, 5, 2, 4. Assuming the cache is initially empty, what is the hit ratio for each of the following algorithms?_
    * _Optimal_ — +3, +4, H4, +5, H3, -5/+2, H3, H4, -3/+1, H4, H4, H2, -1/+5, H2, H4 — 9/15 = 60%
    * Q2: _FIFO_ — +3, +4, H4, +5, H3, -3/+2, -4/+3, -5/+4, -2/+1, H4, H4, -3/+2, -4/+5, H2, -1/+4 — 5/15 - 33%
    * Q3: _LRU_ — +3, +4, H4, +5, H3, -4/+2, H3, -5/+4, -2/+1, H4, H4, -3/+2, -1/+5, H2, H4 —  7/15 = 47%
    * Q4: _LFU_ — +3, +4, H4, +5, H3, -5/+2, H3, H4, -2/+1, H4, H4, -1/+2, -2/+5, -5/+2, H4 — 7/15 = 47%

## Operating systems (OS) overview
* Purpose of an OS
    * Make computer hardware easy to use—e.g., an OS knows how to load an application's executable code from persistent storage (e.g., solid state drive (SSD)) into main memory, initialize the process's address space (code, heap, stack), and make the CPU execute the application's instructions
    * Support multiprocessing—i.e., running multiple applications concurrently
        * Concurrently == switch between multiple tasks during a window of time—e.g., alternate between cooking and setting the table
        * Simultaneously == complete two tasks at the same time—e.g., listen to a podcast while cooking
    * Allocate and manage hardware resources—e.g., decide when/which applications can use the CPU, decide when/which memory applications can use, prevent applications from stealing/accessing another application's CPU time or memory
    * Many OSes also provide a user interface (UI)
* How does the OS fulfill its duties?
    * Mechanisms — fundamental approaches for managing/providing access to hardware resources
        * E.g., system calls, process abstraction
    * Policy — specific ways of employing an approach; different policies make different trade-offs (in terms of efficiency, performance, etc.)
        * E.g., process scheduler

## Accessing hardware
* OS is responsible for allocating/managing the hardware
    * ⇒ applications should **not have unfettered access to hardware**
* _How should applications access the hardware?_
    * Ask the OS for access to the hardware
        * How do we ensure the OS does not "lose control" of the hardware? 
    * Asks the OS to perform an action on the application's behalf
        * How do we ensure this doesn't substantially degrade performance?
* Example: execute an instruction on the CPU
    * Asking the OS to do this on behalf of an application is impractical—OS would need to execute multiple assembly instructions for each assembly instruction the application wants to execute
    * Alternative: allow the application to execute its own instructions on the CPU
        * This is risky—an application may execute an instruction that controls the hardware, e.g., `hlt` (halt) instruction pauses the CPU
    * Alternative: allow the application to execute "safe" instructions on its own on the CPU
* Example: accessing the solid state drive (SSD)
    * Allowing the application to access the SSD directly
        * This is risky—an application may read/write data it should not be able to access
    * Alternative: asking the OS to access the SSD on the application's behalf
        * Latency of accessing SSD (~1 million CPU cycles) far outweighs the extra instructions required for OS to perform the access on the application's behalf
* Mechanisms
    * Limited Direct Execution (LDE)
    * System calls
